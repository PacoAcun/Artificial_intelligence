{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c27b6c90",
      "metadata": {
        "id": "c27b6c90"
      },
      "source": [
        "\n",
        "# Laboratorio: TinyLlama y comparaci√≥n de respuestas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "cc8d8bc1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc8d8bc1",
        "outputId": "0118b48e-d06c-421a-baf7-bc34adf8c302"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch: 2.8.0+cu126 | CUDA disponible: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip -q install --upgrade transformers accelerate bitsandbytes sentencepiece\n",
        "!pip -q install --upgrade huggingface_hub\n",
        "\n",
        "import torch, platform, time, json\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from huggingface_hub import login\n",
        "\n",
        "print(\"Torch:\", torch.__version__, \"| CUDA disponible:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    from torch import cuda\n",
        "    print(\"GPU:\", cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"Usando CPU.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "bf8fbec0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf8fbec0",
        "outputId": "6c72300f-8a0e-43fb-e550-984d843402cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration ready. USE_4BIT = True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if torch.cuda.is_available():\n",
        "    dtype = torch.float16\n",
        "    device_map = \"auto\"\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "else:\n",
        "    dtype = torch.float16\n",
        "    device_map = \"auto\"\n",
        "\n",
        "USE_4BIT = True\n",
        "\n",
        "def build_generator(model_id: str):\n",
        "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True, trust_remote_code=False)\n",
        "    if USE_4BIT:\n",
        "        mdl = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            device_map=device_map,\n",
        "            load_in_4bit=True\n",
        "        )\n",
        "    else:\n",
        "        mdl = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=dtype,\n",
        "            device_map=device_map\n",
        "        )\n",
        "    gen = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=mdl,\n",
        "        tokenizer=tok,\n",
        "        return_full_text=False\n",
        "    )\n",
        "    return gen, tok\n",
        "\n",
        "PROMPT = \"Explain me whats a transformer, assuming im a 5 years old kid\"\n",
        "print(\"Configuration ready. USE_4BIT =\", USE_4BIT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "9a44e691",
      "metadata": {
        "id": "9a44e691"
      },
      "outputs": [],
      "source": [
        "\n",
        "MODEL_IDS = [\n",
        "    \"TinyLlama/TinyLlama_v1.1\",\n",
        "    \"Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct\",\n",
        "    \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "]\n",
        "\n",
        "TEMPERATURE = 0.1\n",
        "MAX_NEW_TOKENS = 200\n",
        "\n",
        "RESULTS = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "8498d933",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8498d933",
        "outputId": "cd4afb2c-82ac-4638-e059-01907040ad0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==============================\n",
            "Cargando modelo: TinyLlama/TinyLlama_v1.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Carga lista en 48.23 s\n",
            "Generando respuesta...\n",
            "Hecho. Prompt tokens: 18 | Output tokens: 201 | Carga: 48.23s | Inferencia: 9.67s\n",
            "\n",
            "--- Respuesta ---\n",
            " , im a 10 year old kid, im a 15 year old kid, im a 20 year old kid, im a 25 year old kid, im a 30 year old kid, im a 35 year old kid, im a 40 year old kid, im a 45 year old kid, im a 50 year old kid, im a 55 year old kid, im a 60 year old kid, im a 65 year old kid, im a 70 year old kid, im a 75 year old kid, im a 80 year old kid, im a 85 year old kid, im a 90 year old kid, im a 95 year old kid, im a 100 year old kid, im a 105 year old kid, im a 107 year old kid, im a 109 year \n",
            "------------------\n",
            "\n",
            "==============================\n",
            "Cargando modelo: Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Carga lista en 10.12 s\n",
            "Generando respuesta...\n",
            "Hecho. Prompt tokens: 18 | Output tokens: 97 | Carga: 10.12s | Inferencia: 4.49s\n",
            "\n",
            "--- Respuesta ---\n",
            " .\n",
            "Explain me whats a transformer, assuming im a 5 years old kid.\n",
            "A transformer is a device that converts alternating current (AC) to direct current (DC) or vice versa. It is used to convert the AC current to DC current.\n",
            "A transformer is a device that converts alternating current (AC) to direct current (DC) or vice versa. It is used to convert the AC current to DC current. \n",
            "------------------\n",
            "\n",
            "==============================\n",
            "Cargando modelo: Qwen/Qwen2.5-0.5B-Instruct\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Carga lista en 6.07 s\n",
            "Generando respuesta...\n",
            "Hecho. Prompt tokens: 15 | Output tokens: 200 | Carga: 6.07s | Inferencia: 11.38s\n",
            "\n",
            "--- Respuesta ---\n",
            "  and i dont know what it is or how to use it.\n",
            "A transformer is a device that converts electrical energy from one type of medium (like electricity) into another type of medium (like heat or light). It's like magic! But you can't do this with your own hands. So, imagine you have a toy box where you keep all the different types of toys. Now, you want to make sure they're safe for everyone in the house. You might decide to put some special lights on the walls to make them look nice. But you don't want to burn any of the toys inside because they could be dangerous. To protect everyone, you need to turn off the lights. But you also want to make sure there are no other dangers too. So, you decide to add more cool things like magnets to make sure everything stays safe.\n",
            "\n",
            "In this way, we can think of a transformer as a magical device that helps us control and protect our homes by making sure we have safety measures \n",
            "------------------\n",
            "\n",
            "Listo. Se generaron 3 respuestas.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from contextlib import nullcontext\n",
        "\n",
        "for model_id in MODEL_IDS:\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"Cargando modelo:\", model_id)\n",
        "    t0 = time.time()\n",
        "    generator, tok = build_generator(model_id)\n",
        "    load_s = time.time() - t0\n",
        "    print(f\"Carga lista en {load_s:.2f} s\")\n",
        "\n",
        "    print(\"Generando respuesta...\")\n",
        "    t1 = time.time()\n",
        "    out = generator(PROMPT, max_new_tokens=MAX_NEW_TOKENS, temperature=TEMPERATURE)\n",
        "    infer_s = time.time() - t1\n",
        "\n",
        "    text = out[0][\"generated_text\"]\n",
        "    prompt_tokens = len(tok.encode(PROMPT))\n",
        "    gen_tokens = len(tok.encode(text))\n",
        "\n",
        "    print(f\"Hecho. Prompt tokens: {prompt_tokens} | Output tokens: {gen_tokens} | \"\n",
        "          f\"Carga: {load_s:.2f}s | Inferencia: {infer_s:.2f}s\")\n",
        "    print(\"\\n--- Respuesta ---\\n\", text[:1000], \"\\n------------------\")\n",
        "\n",
        "    RESULTS.append({\n",
        "        \"model_id\": model_id,\n",
        "        \"prompt\": PROMPT,\n",
        "        \"temperature\": TEMPERATURE,\n",
        "        \"max_new_tokens\": MAX_NEW_TOKENS,\n",
        "        \"prompt_tokens\": prompt_tokens,\n",
        "        \"gen_tokens\": gen_tokens,\n",
        "        \"load_seconds\": round(load_s, 2),\n",
        "        \"infer_seconds\": round(infer_s, 2),\n",
        "        \"response\": text\n",
        "    })\n",
        "\n",
        "print(\"\\nListo. Se generaron\", len(RESULTS), \"respuestas.\")\n",
        "\n",
        "os.makedirs(\"/content\", exist_ok=True)\n",
        "json_path = \"/content/resultados_modelos.json\"\n",
        "\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(RESULTS, f, ensure_ascii=False, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "42570382",
      "metadata": {
        "id": "42570382"
      },
      "outputs": [],
      "source": [
        "\n",
        "CHATGPT_RESPONSE = \"\"\"\n",
        "Okay, imagine this üëá\n",
        "\n",
        "You have a magic story machine called a Transformer.\n",
        "\n",
        "üß† This machine loves words. You can give it a few words ‚Äî like ‚ÄúOnce upon a time‚Äù ‚Äî and it will guess what word should come next, over and over, until it finishes a whole story.\n",
        "\n",
        "But here‚Äôs the cool part:\n",
        "The Transformer doesn‚Äôt just remember the last word you said. It remembers all the words, all at once!\n",
        "It looks at every word in the sentence and thinks:\n",
        "\n",
        "‚ÄúHmm‚Ä¶ which words are most important to understand what comes next?‚Äù\n",
        "\n",
        "That trick is called attention ‚Äî like when you‚Äôre in class and you pay attention to what the teacher says that matters most.\n",
        "\n",
        "So, in simple words:\n",
        "\n",
        "A Transformer is a smart story machine.\n",
        "\n",
        "It reads and writes words by paying attention to all the words together.\n",
        "\n",
        "That‚Äôs how ChatGPT, TinyLlama, and other language models can talk, write, and understand sentences!\n",
        "\n",
        "If you want a fun picture idea:\n",
        "it‚Äôs like a giant team of word detectives, all talking to each other to figure out what word should come next.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "83b5128b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83b5128b",
        "outputId": "867313a1-63e3-47c0-e6db-3796c03840a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Comparaci√≥n (incluye ChatGPT)\n",
            "                                       Modelo  Longitud_respuesta_(palabras)  \\\n",
            "0                    TinyLlama/TinyLlama_v1.1                            131   \n",
            "1  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct                             69   \n",
            "2                  Qwen/Qwen2.5-0.5B-Instruct                            177   \n",
            "3                                     ChatGPT                            183   \n",
            "\n",
            "   Carga_s  Inferencia_s  \n",
            "0    48.23          9.67  \n",
            "1    10.12          4.49  \n",
            "2     6.07         11.38  \n",
            "3      NaN           NaN  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import json, os, textwrap\n",
        "\n",
        "with open(\"/content/resultados_modelos.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    RESULTS = json.load(f)\n",
        "\n",
        "def count_words(s: str) -> int:\n",
        "    return len(s.strip().split())\n",
        "\n",
        "RESULTS.append({\n",
        "    \"model_id\": \"ChatGPT\",\n",
        "    \"prompt\": RESULTS[0][\"prompt\"] if RESULTS else \"Explain me whats a transformer, assuming im a 5 years old kid\",\n",
        "    \"temperature\": None,\n",
        "    \"max_new_tokens\": None,\n",
        "    \"prompt_tokens\": None,\n",
        "    \"gen_tokens\": None,\n",
        "    \"load_seconds\": None,\n",
        "    \"infer_seconds\": None,\n",
        "    \"response\": CHATGPT_RESPONSE.strip()\n",
        "})\n",
        "\n",
        "df_comp = pd.DataFrame([\n",
        "    {\n",
        "        \"Modelo\": r[\"model_id\"],\n",
        "        \"Longitud_respuesta_(palabras)\": count_words(r[\"response\"]),\n",
        "        \"Carga_s\": r.get(\"load_seconds\"),\n",
        "        \"Inferencia_s\": r.get(\"infer_seconds\")\n",
        "    }\n",
        "    for r in RESULTS\n",
        "])\n",
        "\n",
        "print(\"### Comparaci√≥n (incluye ChatGPT)\")\n",
        "print(df_comp)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xDTY2KhJvvAW",
      "metadata": {
        "id": "xDTY2KhJvvAW"
      },
      "source": [
        "## comparativo de las respuestas\n",
        "\n",
        "### Similitudes\n",
        "Pr√°cticamente no existen similitudes en el contenido. La √∫nica similitud superficial es que Qwen y ChatGPT usaron un tono que intenta ser simple (usando palabras como \"magia\" o \"m√°quina de historias\"). Sin embargo, ning√∫n modelo us√≥ met√°foras parecidas. ChatGPT fue el √∫nico que mantuvo exitosamente el nivel de 5 a√±os y present√≥ una estructura clara. Los otros modelos fallaron en el objetivo: TinyLlama_v1.1 no respondi√≥, Doctor-Shotgun fue demasiado t√©cnico y Qwen fue incoherente.\n",
        "\n",
        "### Diferencias\n",
        "La diferencia m√°s grande es la interpretaci√≥n del contexto. ChatGPT fue el √∫nico que entendi√≥ que \"transformer\" se refer√≠a al modelo de Inteligencia Artificial (el tema del laboratorio ). Los modelos Doctor-Shotgun y Qwen alucinaron y asumieron que la pregunta era sobre un transformador el√©ctrico. TinyLlama_v1.1 alucin√≥ de otra manera, simplemente repitiendo variaciones de la edad del prompt. En cuanto a precisi√≥n, Doctor-Shotgun intent√≥ ser t√©cnico pero fue incorrecto (describi√≥ un rectificador AC/DC). ChatGPT fue el m√°s fluido y gramaticalmente correcto, mientras que los otros fueron repetitivos o confusos.\n",
        "\n",
        "### Estilo\n",
        "ChatGPT adopt√≥ claramente un estilo de cuento o analog√≠a, usando ejemplos como \"m√°quina de historias m√°gica\" y \"detectives de palabras\" con un tono docente y amigable. Qwen intent√≥ una analog√≠a (\"caja de juguetes\", \"luces\"), pero el estilo fue confuso y la conexi√≥n con el concepto (incluso el el√©ctrico) era nula. Doctor-Shotgun us√≥ un estilo de definici√≥n t√©cnica, totalmente opuesto al solicitado. TinyLlama_v1.1 no tuvo estilo, solo repiti√≥ texto.\n",
        "\n",
        "### Calidad\n",
        "La calidad de ChatGPT fue muy alta: la respuesta fue clara, coherente, mantuvo el tono adecuado y fue la √∫nica correcta en cuanto al contexto del laboratorio. La calidad de Qwen fue baja; aunque intent√≥ un tono simple, la explicaci√≥n fue incoherente. La calidad de Doctor-Shotgun fue muy baja por ser demasiado t√©cnico, fallar en el tono y ser conceptualmente incorrecto. La calidad de TinyLlama_v1.1 fue nula, pues no produjo ninguna respuesta relevante.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
